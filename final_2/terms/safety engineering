{"batchcomplete":"","query":{"normalized":[{"from":"safety_engineering","to":"Safety engineering"}],"pages":{"29278":{"pageid":29278,"ns":0,"title":"Safety engineering","revisions":[{"contentformat":"text/x-wiki","contentmodel":"wikitext","*":"{{More footnotes|date=January 2011}}\n[[File:ISS_impact_risk.jpg|thumb|300px|right|NASA's illustration showing high impact risk areas for the International Space Station]]\n'''Safety engineering''' is an engineering discipline which assures that engineered systems provide acceptable levels of [[safety]]. It is strongly related to [[industrial engineering]]/[[systems engineering]],  and the subset [[system safety]] engineering. Safety engineering assures that a [[life-critical system]] behaves as needed, even when components fail.\n\n==Analysis techniques==\nAnalysis techniques can be split into two categories: qualitative and quantitative methods. Both approaches share the goal of finding causal dependencies between a hazard on system level and failures of individual components. Qualitative approaches focus on the question \"What must go wrong, such that a system hazard may occur?\", while quantitative methods aim at providing estimations about probabilities, rates and/or severity of consequences.\n\n[[File:Risk Cost Analysis.jpg|thumb|Risk vs Cost/Complexity<ref>Kokcharov I. Structural Safety http://www.kokch.kts.ru/me/t6/SIA_6_Structural_Safety.pdf</ref>]]\n\nThe complexity of the technical systems such as Improvements of Design and Materials, Planned Inspections, Fool-proof design, and Backup Redundancy decreases risk and increases the cost. The risk can be decreased to ALARA (as low as reasonably achievable) or ALAPA (as low as practically achievable) levels.\n\nTraditionally, safety analysis techniques rely solely on skill and expertise of the safety engineer. In the last decade model-based approaches have become prominent. In contrast to traditional methods, model-based techniques try to derive relationships between causes and consequences from some sort of model of the system.\n\n===Traditional methods for safety analysis===\nThe two most common fault modeling techniques are called [[failure mode and effects analysis]] and [[fault tree analysis]]. These techniques are just ways of finding problems and of making plans to cope with failures, as in [[probabilistic risk assessment]]. One of the earliest complete studies using this technique on a commercial nuclear plant was the [[WASH-1400]] study, also known as the Reactor Safety Study or the Rasmussen Report.\n\n====Failure modes and effects analysis====\n{{Main|Failure mode and effects analysis}}\nFailure Mode and Effects Analysis (FMEA) is a bottom-up, [[inductive reasoning|inductive]] analytical method which may be performed at either the functional or piece-part level. For functional FMEA, failure modes are identified for each function in a system or equipment item, usually with the help of a functional [[block diagram]]. For piece-part FMEA, failure modes are identified for each piece-part component (such as a valve, connector, resistor, or diode). The effects of the failure mode are described, and assigned a probability based on the [[failure rate]] and failure mode ratio of the function or component.   This quantiazation is difficult for software ---a bug exists or not, and the failure models used for hardware components do not apply.  Temperature and age and manufacturing variability affect a resistor; they do not affect software.\n\nFailure modes with identical effects can be combined and summarized in a Failure Mode Effects Summary. When combined with criticality analysis, FMEA is known as [[Failure Mode, Effects, and Criticality Analysis]] or FMECA, pronounced \"fuh-MEE-kuh\".\n\n====Fault tree analysis====\n{{Main|Fault tree analysis}}\nFault tree analysis (FTA) is a top-down, [[deductive reasoning|deductive]] analytical method.  In FTA, initiating primary events such as component failures, human errors, and external events are traced through [[Boolean logic]] gates to an undesired top event such as an aircraft crash or nuclear reactor core melt. The intent is to identify ways to make top events less probable, and verify that safety goals have been achieved.\n\n[[File:Fault tree.svg|thumb|A fault tree diagram]]\n\nFault trees are a logical inverse of success trees, and may be obtained by applying [[de Morgan's laws|de Morgan's theorem]] to success trees (which are directly related to [[reliability block diagram]]s).\n\nFTA may be qualitative or quantitative. When failure and event probabilities are unknown, qualitative fault trees may be analyzed for minimal cut sets. For example, if any minimal cut set contains a single base event, then the top event may be caused by a single failure. Quantitative FTA is used to compute top event probability, and usually requires computer software such as CAFTA from the [[Electric Power Research Institute]] or [[SAPHIRE]] from the [[Idaho National Laboratory]].\n\nSome industries use both fault trees and [[event tree]]s. An event tree starts from an undesired initiator (loss of critical supply, component failure etc.) and follows possible further system events through to a series of final consequences. As each new event is considered, a new node on the tree is added with a split of probabilities of taking either branch. The probabilities of a range of \"top events\" arising from the initial event can then be seen.\n\n==Safety certification==\nUsually a failure in safety-[[product certification|certified]] systems is acceptable{{by whom|date=April 2015}} if, on average, less than one life per  10<sup>9</sup> hours of continuous operation is lost to failure.{as per FAA document AC 25.1309-1A} Most Western [[nuclear reactors]], medical equipment, and commercial [[aircraft]] are certified{{by whom|date=April 2015}} to this level.{{citation needed|date=April 2015}} The cost versus loss of lives has been considered appropriate at this level (by [[FAA]] for aircraft systems under [[Federal Aviation Regulations]]).<ref>{{cite book \n |     author = ANM-110\n |      title = System Design and Analysis\n |    section = \n | sectionurl = \n |    version = \n |  publisher = [[Federal Aviation Administration]]\n |       year = 1988\n |        url = http://www.faa.gov/documentLibrary/media/Advisory_Circular/AC%2025.1309-1A.pdf\n |     format = pdf\n |         id = Advisory Circular AC&nbsp;25.1309-1A\n | accessdate = 2011-02-20\n |      quote = \n |       page = \n |      pages =\n |        ref =\n }}</ref><ref>{{cite book \n |        last = S\u201318\n |       first = \n | author-link = \n |   coauthors = \n |       title = Guidelines for Development of Civil Aircraft and Systems\n |     section = \n |  sectionurl = \n |     version = \n |   publisher = [[Society of Automotive Engineers]]\n |        year = 2010\n |         url = http://standards.sae.org/arp4754a\n |      format = \n |          id = ARP4754A\n |        isbn = \n |  accessdate = \n |       quote = \n |        page = \n |       pages =\n |         ref =\n }}\n</ref><ref>{{cite book \n |        last = S\u201318\n |       first = \n | author-link = \n |   coauthors = \n |       title = Guidelines and methods for conducting the safety assessment process on civil airborne systems and equipment\n |     section = \n |  sectionurl = \n |     version = \n |   publisher = [[Society of Automotive Engineers]]\n |        year = 1996\n |         url = http://www.sae.org/technical/standards/ARP4761\n |      format = \n |          id = ARP4761\n |        isbn = \n |  accessdate = \n |       quote = \n |        page = \n |       pages =\n |         ref =\n }}\n</ref>\n\n==Preventing failure==\n[[File:Survival redundancy.svg|thumbnail|A [[NASA]] graph shows the relationship between the survival of a crew of astronauts and the amount of [[redundancy (engineering)|redundant]] equipment in their spacecraft (the \"MM\", Mission Module).]]\n\nOnce a failure mode is identified, it can usually be mitigated by adding extra or redundant equipment to the system. For example, nuclear reactors contain dangerous [[radiation]], and nuclear reactions can cause so much [[heat]] that no substance might contain them. Therefore, reactors have emergency core cooling systems to keep the temperature down, shielding to contain the radiation, and engineered barriers (usually several, nested, surmounted by a [[containment building]]) to prevent accidental leakage. Safety-critical systems are commonly required to permit no [[single point of failure|single event or component failure]] to result in a catastrophic failure mode.\n\nMost [[biology|biological]] organisms have a certain amount of redundancy: multiple organs, multiple limbs, etc.\n\nFor any given failure, a fail-over or redundancy can almost always be designed and incorporated into a system.\n\nThere are two categories of techniques to reduce the probability of failure:\nFault avoidance techniques increase the reliability of individual items (increased design margin, de-rating, etc.).\nFault tolerance techniques increase the reliability of the system as a whole (redundancies, barriers, etc.).<ref>\nTommaso Sgobba.\n[http://www.spacesafetymagazine.com/spaceflight/commercial-spaceflight/commercial-space-safety-standards-lets-not-re-invent-wheel/ \"Commercial Space Safety Standards: Let\u2019s Not Re-Invent the Wheel\"].\n2015.\n</ref>\n\n== Safety and reliability ==\n{{details|Inherent safety}}{{details|Reliability engineering}}\n\nSafety engineering and reliability engineering have much in common, but safety is not reliability.  If a medical device fails, it should fail safely; other alternatives will be available to the surgeon.  If the engine on a single-engine aircraft fails, there is no backup.  Electrical power grids are designed for both safety and reliability; telephone systems are designed for reliability, which becomes a safety issue when emergency (e.g. US \"911\") calls are placed.\n\n[[Probabilistic risk assessment]] has created a close relationship between safety and reliability. Component reliability, generally defined in terms of component [[failure rate]], and external event probability are both used in quantitative safety assessment methods such as FTA. Related probabilistic methods are used to determine system [[Mean time between failures|Mean Time Between Failure (MTBF)]], system availability, or probability of mission success or failure. Reliability analysis has a broader scope than safety analysis, in that non-critical failures are considered. On the other hand, higher failure rates are considered acceptable for non-critical systems.\n\nSafety generally cannot be achieved through component reliability alone. Catastrophic failure probabilities of 10<sup>\u22129</sup> per hour correspond to the failure rates of very simple components such as  [[resistor]]s or [[capacitor]]s. A complex system containing hundreds or thousands of components might be able to achieve a MTBF of 10,000 to 100,000 hours, meaning it would fail at 10<sup>\u22124</sup> or 10<sup>\u22125</sup> per hour. If a system failure is catastrophic, usually the only practical way to achieve 10<sup>\u22129</sup> per hour failure rate is through redundancy.\n\nWhen adding equipment is impractical (usually because of expense), then the least expensive form of design is often \"inherently fail-safe\". That is, change the system design so its failure modes are not catastrophic. Inherent fail-safes are common in medical equipment, traffic and railway signals, communications equipment, and safety equipment.\n\nThe typical approach is to arrange the system so that ordinary single failures cause the mechanism to shut down in a safe way (for nuclear power plants, this is termed a [[Passive nuclear safety|passively safe]] design, although more than ordinary failures are covered). Alternately, if the system contains a hazard source such as a battery or rotor, then it may be possible to remove the hazard from the system so that its failure modes cannot be catastrophic. The U.S. Department of Defense Standard Practice for System Safety (MIL\u2013STD\u2013882) places the highest priority on elimination of hazards through design selection.<ref>{{cite book \n |        last = \n |       first = \n | author-link = Air Force Materiel Command Safety Office \n |   coauthors = \n |       title = Standard Practice for System Safety\n |     section = \n |  sectionurl = \n |     version = E\n |   publisher = [[United States Department of Defense|U.S. Department of Defense]]\n |        year = 1998\n |         url = \nhttps://acc.dau.mil/adl/en-US/683694/file/75173/MIL-STD-882E%20Final%202012-05-11.pdf\n |      format = pdf\n |          id = MIL-STD-882\n |  accessdate = 2012-05-11\n |       quote = \n |        page = \n |       pages =\n |         ref =\n }}</ref>\n\nOne of the most common fail-safe systems is the overflow tube in baths and kitchen sinks. If the valve sticks open, rather than causing an overflow and damage, the tank spills into an overflow. Another common example is that in an [[elevator]] the cable supporting the car keeps [[spring-loaded brake]]s open. If the cable breaks, the brakes grab rails, and the elevator cabin does not fall.\n\nSome systems can never be made fail safe, as continuous availability is needed. For example, loss of engine thrust in flight is dangerous. Redundancy, fault tolerance, or recovery procedures are used for these situations (e.g. multiple independent controlled and fuel fed engines). This also makes the system less sensitive for the reliability prediction errors or quality induced uncertainty for the separate items. On the other hand, failure detection & correction and avoidance of common cause failures becomes here increasingly important to ensure system level reliability.<ref>{{cite book \n |        last = Bornschlegl\n |       first = Susanne\n | author-link = \n |   coauthors = \n |       title = Ready for SIL 4: Modular Computers for Safety-Critical Mobile Applications\n |     section = \n |  sectionurl = \n |     version = \n |   publisher = MEN Mikro Elektronik\n |        year = 2012\n |         url = \nhttps://www.menmicro.com/downloads/search/dl/sk/%22White%20Paper%3A%20Ready%20for%20SIL4%3A%20Modular%20Computers%20for%20Safety-Critical%20Mobile%20Applications%22/dx/1/\n |      format = pdf\n |          id = \n |  accessdate = 2015-09-21\n |       quote = \n |        page = \n |       pages =\n |         ref =\n }}</ref>\n\n==See also==\n{{div col|3}}\n* [[ARP4761]]\n* [[Earthquake engineering]]\n* [[Effective safety training]]\n* [[Forensic engineering]]\n* [[Hazard and operability study]]\n* [[IEC 61508]]\n* [[Loss-control consultant]]\n* [[Nuclear safety]]\n* [[Occupational medicine]]\n* [[Occupational safety and health]]\n* [[Process safety management]]\n* [[Reliability engineering]]\n* [[Risk assessment]]\n* [[Risk management]]\n* [[Safety life cycle]]\n* [[Zonal safety analysis]]\n\n===Associations===\n* [[Institute of Industrial Engineers]]\n{{div col end}}\n==References==\n\n===Notes===\n{{reflist}}\n\n===Sources===\n* {{cite book|first=Frank|last=Lees|title=Loss Prevention in the Process Industries|edition=3|publisher=Elsevier|year=2005|isbn=978-0-7506-7555-0|authorlink=Frank Lees}}\n* {{cite book|first=Trevor|last=Kletz|title=Cheaper, safer plants, or wealth and safety at work: notes on inherently safer and simpler plants|publisher=I.Chem.E.|year=1984|isbn=0-85295-167-1|authorlink=Trevor Kletz}}\n* {{cite book|first=Trevor|last=Kletz|edition=3|title=An Engineer\u2019s View of Human Error |publisher=I.Chem.E.|year=2001|isbn=0-85295-430-1|authorlink=Trevor Kletz}}\n* {{cite book|first=Trevor|last=Kletz|edition=4|title=HAZOP and HAZAN |publisher=Taylor & Francis|year=1999|isbn=0-85295-421-2|authorlink=Trevor Kletz}}\n* {{cite book\n  | first = Robyn R.\n  | last = Lutz\n  | title = Software Engineering for Safety: A Roadmap\n  | series = The Future of Software Engineering\n  | publisher = ACM Press\n  | year = 2000\n  | url = http://www.cs.ucl.ac.uk/staff/A.Finkelstein/fose/finallutz.pdf\n  | isbn = 1-58113-253-0\n  | accessdate = 31 August 2006\n}}\n* {{cite journal\n  | first = Lars\n  | last=Grunske\n  | first2 = Bernhard\n  | last2 = Kaiser\n  | first3 = Ralf H.\n  | last3 = Reussner\n  | title = Specification and Evaluation of Safety Properties in a Component-based Software Engineering Process\n  | publisher = Springer\n  | year = 2005\n  | citeseerx = 10.1.1.69.7756\n}}\n* {{cite book\n  | author = US DOD\n  | authorlink = United States Department of Defense\n  | title = Standard Practice for System Safety\n  | publisher = US DOD\n  | date = 10 February 2000\n  | location = Washington, DC\n  | url = http://www.faa.gov/regulations_policies/handbooks_manuals/aviation/risk_management/ss_handbook/media/app_h_1200.pdf\n  | accessdate = 7 September 2013\n  | id = MIL-STD-882D }}\n* {{cite book\n  | author = US FAA\n  | authorlink = Federal Aviation Administration\n  | title = System Safety Handbook\n  | publisher =US FAA\n  | date = 30 December 2000\n  | location = Washington, DC\n  | url = http://www.faa.gov/regulations_policies/handbooks_manuals/aviation/risk_management/ss_handbook/\n  | accessdate = 7 September 2013\n}}\n* {{cite book\n  | author = NASA\n  | url= http://nodis3.gsfc.nasa.gov/displayDir.cfm?Internal_ID=N_PR_8000_004A_\n  | title = Agency Risk Management Procedural Requirements\n  | publisher = NASA\n  | authorlink = NASA\n  | id = NPR 8000.4A\n  | date = 16 December 2008\n}}\n* {{cite book\n  | first = Nancy\n  | last = Leveson\n  | title = Engineering a Safer World - Systems Thinking Applied To Safety\n  | series = Engineering Systems\n  | publisher = The MIT Press\n  | year = 2011\n  | url = http://sunnyday.mit.edu/safer-world/index.html\n  | isbn = 978-0-262-01662-9\n  | accessdate = 3 July 2012\n}}\n\n== External links ==\n* [http://www.apd.army.mil/jw2/xmldemo/p385_16/head.asp U.S. Army Pamphlet 385-16 System Safety Management Guide]\n\n{{Systems Engineering}}\n{{Technology-footer}}\n{{Occupational safety and health}}\n\n[[Category:Safety engineering|*]]\n[[Category:Risk]]\n[[Category:Design for X]]\n[[Category:Reliability engineering]]"}]}}}}