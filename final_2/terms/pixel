{"batchcomplete":"","query":{"normalized":[{"from":"pixel","to":"Pixel"}],"pages":{"23665":{"pageid":23665,"ns":0,"title":"Pixel","revisions":[{"contentformat":"text/x-wiki","contentmodel":"wikitext","*":"{{about|the picture element|other uses|}}\n[[File:Pixel-example.png|right|frame|This example shows an image with a portion greatly enlarged, in which the individual pixels are rendered as small squares and can easily be seen.]]\n[[File:Closeup of pixels.JPG|thumb|right|220px|A photograph of sub-pixel display elements on a laptop's [[Liquid crystal display|LCD]] screen]]\n\nIn [[digital imaging]], a '''pixel''', '''pel''',<ref>{{cite book |first=J. D. |last=Foley |first2=A. |last2=Van Dam |title=Fundamentals of Interactive Computer Graphics |year=1982 |location=Reading, MA |publisher=Addison-Wesley |isbn=0201144689 }}</ref> '''dots''', or '''picture element'''<ref>{{Cite book | author=Rudolf F. Graf | year=1999 | publisher=Newnes | title=Modern Dictionary of Electronics | location=Oxford | isbn=0-7506-4331-5|page=569|url=https://books.google.com/books?id=o2I1JWPpdusC&pg=PA569}}</ref> is a physical point in a [[Raster graphics|raster image]], or the smallest addressable element in an [[all points addressable]] [[display device]]; so it is the smallest controllable element of a picture represented on the screen. The address of a pixel corresponds to its physical coordinates. [[LCD]] pixels are manufactured in a two-dimensional grid, and are often represented using dots or squares, but [[cathode ray tube|CRT]] pixels correspond to their timing mechanisms and sweep rates.\n\nEach pixel is a [[Sampling (signal processing)|sample]] of an original image; more samples typically provide more accurate representations of the original. The [[Intensity (physics)|intensity]] of each pixel is variable. In color imaging systems, a color is typically represented by three or four component intensities such as [[RGB color model|red, green, and blue]], or [[CMYK color model|cyan, magenta, yellow, and black]].\n\nIn some contexts (such as descriptions of camera sensors), the term ''pixel'' is used to refer to a single scalar element of a multi-component representation (more precisely called a ''photosite'' in the camera sensor context, although the neologism ''[[wikt:sensel|sensel]]'' is sometimes used to describe the elements of a digital camera's sensor),<ref>{{cite book | title = New Acquisition Techniques for Real Objects and Light Sources in Computer Graphics | author = Michael Goesele | publisher = Books on Demand | year = 2004 | isbn = 3-8334-1489-8 | url = https://books.google.com/books?id=ZTJJ8QzNv1wC&pg=PA33&dq=sensel+sensor-element }}</ref> while in yet other contexts the term may be used to refer to the set of component intensities for a spatial position, though this is more accurately termed a sample. Drawing a distinction between pixels, photosite and samples avoids confusion when describing color systems that use [[chroma subsampling]] or cameras that use [[Bayer filter]] to produce color components via upsampling.\n\nThe word ''pixel'' is based on a contraction of ''pix'' (from word \"pictures\", where it is shortened to \"pics\", and \"cs\" in \"pics\" sounds like \"x\") and ''el'' (for \"element\"); similar formations with '<nowiki/>''el''' include the words ''[[voxel]]'',<ref name=\"JF\">{{cite book | last = Foley | first = James D. |author2=Andries van Dam |author3=John F. Hughes |author4=Steven K. Feiner | title = [[Computer Graphics: Principles and Practice]] | publisher=[[Addison-Wesley]]|year=1990| isbn=0-201-12110-7 | series= The Systems Programming Series | chapter=Spatial-partitioning representations; Surface detail | quote= These cells are often called ''voxels'' (volume elements), in analogy to pixels.}}</ref> [[Texel (graphics)|''texel'']]<ref name=JF/> and ''maxel'' (for ''magnetic pixel'').<ref>{{Cite web| last = By| title = Just When You Thought Magnets Weren\u2019t Magic; Magnets Are Mechanisms| work = Hackaday| accessdate = 2016-03-22| url = http://hackaday.com/2016/03/21/just-when-you-thought-magnets-werent-magic-magnets-are-mechanisms/}}</ref>\n\n== Etymology ==\nThe word \"pixel\" was first published in 1965 by [[Frederic C. Billingsley]] of [[Jet Propulsion Laboratory|JPL]], to describe the picture elements of video images from space probes to the Moon and Mars.<ref>Fred C. Billingsley, \"Processing Ranger and Mariner Photography,\" in ''Computerized Imaging Techniques, Proceedings of SPIE'', Vol. 0010, pp. XV-1\u201319, Jan. 1967 (Aug. 1965, San Francisco).</ref> Billingsley had learned the word from Keith E. McFarland, at the Link Division of General Precision in [[Palo Alto, California|Palo Alto]], who in turn said he did not know where it originated. McFarland said simply it was \"in use at the time\" (circa 1963).<ref name=lyon>Lyon, Richard F. (2006). [http://www.foveon.com/files/ABriefHistoryofPixel2.pdf A brief history of 'pixel']. IS&T/SPIE Symposium on Electronic Imaging.</ref>\n\nThe word is a combination of ''pix'', for picture, and ''element''.  The word ''pix'' appeared in ''[[Variety (magazine)|Variety]]'' magazine headlines in 1932, as an abbreviation for the word ''pictures'', in reference to movies.<ref name=onetdict>{{cite web | title = Online Etymology Dictionary | url = http://www.etymonline.com/index.php?search=pixel}}</ref> By 1938, \"pix\" was being used in reference to still pictures by photojournalists.<ref name=lyon/>\n\nThe concept of a \"picture element\" dates to the earliest days of television, for example as \"''Bildpunkt''\" (the German word for ''pixel'', literally 'picture point') in the 1888 German patent of [[Paul Nipkow]]. According to various etymologies, the earliest publication of the term ''picture element'' itself was in ''[[Wireless World]]'' magazine in 1927,<ref>[http://query.nytimes.com/gst/fullpage.html?res=990CE3DB1E31F931A35757C0A963958260 \"On language; Modem, I'm Odem\"], ''[[The New York Times]]'', April 2, 1995. Accessed April 7, 2008.</ref> though it had been used earlier in various U.S. patents filed as early as 1911.<ref>Sinding-Larsen, Alf [http://www.google.com/patents?vid=USPAT1175313&id=Df9EAAAAEBAJ&dq=%22picture+element%22&as_drrb_ap=b Transmission of Pictures of Moving Objects], US Patent 1,175,313, issued March 14, 1916.</ref>\n\nSome authors explain ''pixel'' as ''picture cell,'' as early as 1972.<ref>{{cite journal | journal = IEEE Trans. Comput. | title = Techniques for Change Detection | author = Robert L. Lillestrand | volume = C-21 | issue = 7 | year = 1972}}</ref> In graphics and in image and video processing, ''pel'' is often used instead of ''pixel''.<ref>Lewis, Peter H. (February 12, 1989). [http://www.nytimes.com/1989/02/12/business/the-executive-computer-compaq-sharpens-its-video-option.html The Executive Computer; Compaq Sharpens Its Video Option]. ''[[The New York Times]]''.</ref> For example, IBM used it in their Technical Reference for the [[IBM Personal Computer|original PC]].\n\n[[Pixilation]], spelled with a second ''i'', is an unrelated filmmaking technique that dates to the beginnings of cinema, in which live actors are posed frame by frame and photographed to create stop-motion animation. An archaic British word meaning \"possession by spirits ([[pixie]]s),\" the term has been used to describe the animation process since the early 1950s; various animators, including [[Norman McLaren]] and [[Grant Munro (filmmaker)|Grant Munro]], are credited with popularizing it.<ref>[https://books.google.com/books?id=8T7VLeWAnq8C&pg=SA2-PA17&dq=norman+mclaren+pixilation&hl=en&sa=X&ei=YoPuUp_DFeGIygHgwYCoCg&ved=0CDAQ6AEwAQ#v=onepage&q=norman%20mclaren%20pixilation&f=false Frame by Frame Stop Motion: NonTraditional Approaches to Stop Motion Animation - Tom Gasek - Google Books<!-- Bot generated title -->]</ref>\n\n==Technical==\n[[Image:ReconstructionsFromPixels.png|right|frame|A pixel does not need to be rendered as a small square. This image shows alternative ways of reconstructing an image from a set of pixel values, using dots, lines, or smooth filtering.]]\n\n{{nowrap|A pixel is generally}} thought of as the smallest single component of a digital image. However, the definition is highly context-sensitive. For example, there can be \"[[CMYK|printed pixels]]\" in a page, or pixels carried by electronic signals, or represented by digital values, or pixels on a display device, or pixels in a [[digital camera]] (photosensor elements). This list is not exhaustive and, depending on context, synonyms include pel, sample, byte, bit, dot, and spot. ''Pixels'' can be used as a unit of measure such as: 2400 pixels per inch, 640 pixels per line, or spaced 10 pixels apart.\n\nThe measures [[dots per inch]] (dpi) and [[pixels per inch]] (ppi) are sometimes used interchangeably, but have distinct meanings, especially for printer devices, where dpi is a measure of the printer's density of dot (e.g. ink droplet) placement.<ref>{{cite book | title = The Magic of Digital Printing\n  | author=Derek Doeffinger | publisher=Lark Books\n  | year=2005 | isbn=1-57990-689-3 | page=24\n  | url = https://books.google.com/books?id=s2hIx1amJUcC&dq=printer+dots-per-inch+pixels-per-inch\n}}</ref>  For example, a high-quality photographic image may be printed with 600 ppi on a 1200 dpi inkjet printer.<ref>{{cite web | url = http://www.clarkvision.com/imagedetail/printer-ppi/ | title =  Experiments with Pixels Per Inch (PPI) on Printed Image Sharpness | work = ClarkVision.com | date = July 3, 2005}}</ref>  Even higher dpi numbers, such as the 4800 dpi quoted by printer manufacturers since 2002, do not mean much in terms of achievable [[image resolution|resolution]].<ref>{{cite book | title=Mastering Digital Printing\n  | author=Harald Johnson | publisher=Thomson Course Technology\n  | year=2002 | isbn=1-929685-65-3 | page=40\n  | url=https://books.google.com/books?id=wto19gxFyfQC&dq=inkjet+printer+4800+dpi+addressability }}</ref>\n\nThe more pixels used to represent an image, the closer the result can resemble the original. The number of pixels in an image is sometimes called the resolution, though resolution has a more specific definition. Pixel counts can be expressed as a single number, as in a \"three-megapixel\" digital camera, which has a nominal three million pixels, or as a pair of numbers, as in a \"640 by 480 display\", which has 640 pixels from side to side and 480 from top to bottom (as in a [[Video Graphics Array|VGA]] display), and therefore has a total number of 640\u00d7480 = 307,200 pixels or 0.3 megapixels.\n\nThe pixels, or color samples, that form a digitized image (such as a [[JPEG]] file used on a web page) may or may not be in one-to-one [[bijection|correspondence]] with screen pixels, depending on how a computer displays an image. In computing, an image composed of pixels is known as a ''[[bitmap|bitmapped image]]'' or a ''[[raster graphics|raster image]]''. The word ''raster'' originates from [[raster scan|television scanning]] patterns, and has been widely used to describe similar [[halftone]] printing and storage techniques.\n\n===Sampling patterns===\nFor convenience, pixels are normally arranged in a [[regular grid|regular two-dimensional grid]]. By using this arrangement, many common operations can be implemented by uniformly applying the same operation to each pixel independently. Other arrangements of pixels are possible, with some sampling patterns even changing the shape (or [[Convolution kernel|kernel]]) of each pixel across the image. For this reason, care must be taken when acquiring an image on one device and displaying it on another, or when converting image data from one pixel format to another.\n\nFor example:\n[[Image:Wikipedia ClearType.png|thumb|right|250px|Text rendered using [[ClearType]]]]\n* [[Liquid crystal display|LCD screens]] typically use a staggered grid, where the red, green, and blue components are sampled at slightly different locations.  [[Subpixel rendering]] is a technology which takes advantage of these differences to improve the rendering of text on LCD screens.\n* The vast majority of color digital cameras use a [[Bayer filter]], resulting in a regular grid of pixels where the ''color'' of each pixel depends on its position on the grid.\n* A [[clipmap]] uses a hierarchical sampling pattern, where the size of the [[support (mathematics)|support]] of each pixel depends on its location within the hierarchy.\n* Warped grids are used when the underlying geometry is non-planar, such as images of the earth from space.<ref>{{cite web |url=http://staff.utia.cas.cz/zitova/registration.htm |title=Image registration of blurred satellite images |accessdate=2008-05-09}}</ref>\n* The use of non-uniform grids is an active research area, attempting to bypass the traditional [[Nyquist rate|Nyquist limit]].<ref>{{cite web |url=http://www.sciencedirect.com/science/article/B6V14-3YXB1T5-8/2/15b559f2aefbc7bdc54c55df148c2374 |title=ScienceDirect - Pattern Recognition: Image representation by a new optimal non-uniform morphological sampling: |accessdate=2008-05-09}}</ref>\n* Pixels on computer monitors are normally \"square\" (this is, having equal horizontal and vertical sampling pitch); pixels in other systems are often \"rectangular\" (that is, having unequal horizontal and vertical sampling pitch \u2013 oblong in shape), as are [[digital video]] formats with diverse [[Pixel aspect ratio|aspect ratios]], such as the [[anamorphic widescreen]] formats of the [[Rec. 601]] digital video standard.\n\n===Resolution of computer monitors===\nComputers can use pixels to display an image, often an abstract image that represents a [[GUI]]. The resolution of this image is called the display resolution and is determined by the [[video card]] of the computer. [[LCD]] monitors also use pixels to display an image, and have a [[native resolution]]. Each pixel is made up of [[Triad (computers)|triads]], with the number of these triads determining the native resolution. On some [[Cathode ray tube|CRT]] monitors, the beam sweep rate may be fixed, resulting in a fixed native resolution. Most CRT monitors do not have a fixed beam sweep rate, meaning they do not have a native resolution at all - instead they have a set of resolutions that are equally well supported.\nTo produce the sharpest images possible on an LCD, the user must ensure the display resolution of the computer matches the native resolution of the monitor.\n\n===Resolution of telescopes===\nThe pixel scale used in astronomy is the angular distance between two objects on the sky that fall one pixel apart on the detector (CCD or infrared chip). The scale ''s'' measured in [[radian]]s is the ratio of the pixel spacing ''p'' and [[focal length]] ''f'' of the preceding optics, ''s''=''p/f''. (The focal length is the product of the [[F-number|focal ratio]] by the diameter of the associated lens or mirror.)\nBecause ''p'' is usually expressed in units of [[Arcsecond#Symbols and abbreviations|arcseconds]] per pixel, because 1 radian equals ''180/\u03c0*3600\u2248206,265'' arcseconds, and because diameters are often given in millimeters and pixel sizes in micrometers which yields another factor of 1,000, the formula is often quoted as ''s=206p/f''.\n\n=== Bits per pixel ===\n\n{{Main|Color depth}}\n\nThe number of distinct colors that can be represented by a pixel depends on the number of bits per pixel (bpp). A 1 bpp image uses 1-bit for each pixel, so each pixel can be either on or off. Each additional bit doubles the number of colors available, so a 2 bpp image can have 4 colors, and a 3 bpp image can have 8 colors:\n* 1 bpp, 2<sup>1</sup> = 2 colors ([[binary image|monochrome]])\n* 2 bpp, 2<sup>2</sup> = 4 colors\n* 3 bpp, 2<sup>3</sup> = 8 colors\n...\n* 8 bpp, 2<sup>8</sup> = 256 colors\n* 16 bpp, 2<sup>16</sup> = 65,536 colors (\"[[Highcolor]]\" )\n* 24 bpp, 2<sup>24</sup> = 16,777,216 colors (\"[[True Color|Truecolor]]\")\n\nFor color depths of 15 or more bits per pixel, the depth is normally the sum of the bits allocated to each of the red, green, and blue components. [[Highcolor]], usually meaning 16 bpp, normally has five bits for red and blue, and six bits for green, as the human eye is more sensitive to errors in green than in the other two primary colors. For applications involving transparency, the 16 bits may be divided into five bits each of red, green, and blue, with one bit left for transparency. A 24-bit depth allows 8 bits per component. On some systems, 32-bit depth is available: this means that each 24-bit pixel has an extra 8 bits to describe its [[Opacity (optics)|opacity]] (for purposes of combining with another image).\n\n=== Subpixels ===\n[[Image:Pixel geometry 01 Pengo.jpg|thumb|200px|Geometry of color elements of various CRT and LCD displays; [[phosphor]] dots in a color CRTs display (top row) bear no relation to pixels or subpixels.]]\n\nMany display and image-acquisition systems are, for various reasons, not capable of displaying or sensing the different [[Channel (digital image)|color channels]] at the same site. Therefore, the pixel grid is divided into single-color regions that contribute to the displayed or sensed color when viewed at a distance. In some displays, such as LCD, LED, and plasma displays, these single-color regions are separately addressable elements, which have come to be known as '''subpixels'''.<ref>{{cite web|url=http://dictionary.reference.com/browse/pixel|title=''Subpixel'' in Science|publisher=[[dictionary.com]]|accessdate=4 July 2015}}</ref> For example, [[LCD]]s typically divide each pixel horizontally into three subpixels. When the square pixel is divided into three subpixels, each subpixel is necessarily rectangular. In display industry terminology, subpixels are often referred to as ''pixels'',{{by whom|date=July 2015}} as they are the basic addressable elements in a viewpoint of hardware, and hence ''pixel circuits'' rather than ''subpixel circuits'' is used.\n\nMost digital camera [[image sensor]]s use single-color sensor regions, for example using the [[Bayer filter]] pattern, and in the camera industry these are known as ''pixels'' just like in the display industry, not ''subpixels''.\n\nFor systems with subpixels, two different approaches can be taken:\n*The subpixels can be ignored, with full-color pixels being treated as the smallest addressable imaging element; or\n*The subpixels can be included in rendering calculations, which requires more analysis and processing time, but can produce apparently superior images in some cases.\n\nThis latter approach, referred to as [[subpixel rendering]], uses knowledge of [[pixel geometry]] to manipulate the three colored subpixels separately, producing an increase in the apparent resolution of color displays. While [[Cathode Ray Tube|CRT]] displays use red-green-blue-masked phosphor areas, dictated by a mesh grid called the shadow mask, it would require a difficult calibration step to be aligned with the displayed pixel raster, and so CRTs do not currently use subpixel rendering.\n\nThe concept of subpixels is related to [[Sample (graphics)|samples]].\n\n== Megapixel ==<!--This section is linked from redirect [[Megapixel]]-->\n[[File:Sensoraufl\u00f6sungen.svg|thumb|Diagram of common sensor resolutions of digital cameras including megapixel values]]\n[[File:Megapixels.JPG|thumb|Marking on a camera phone that has about 2 million effective pixels.]]\n\nA megapixel (MP) is a million pixels; the term is used not only for the number of pixels in an image, but also to express the number of [[image sensor]] elements of [[digital camera]]s or the number of display elements of [[Computer display|digital displays]]. For example, a camera that makes a 2048\u00d71536 pixel image (3,145,728 finished image pixels) typically uses a few extra rows and columns of sensor elements and is commonly said to have \"3.2 megapixels\" or \"3.4 megapixels\", depending on whether the number reported is the \"effective\" or the \"total\" pixel count.<ref>[http://www.dpreview.com/news/2001/8/1/jciamegapixel Now a megapixel is really a megapixel]</ref>\n\nDigital cameras use photosensitive electronics, either [[charge-coupled device]] (CCD) or [[CMOS|complementary metal\u2013oxide\u2013semiconductor]] (CMOS) image sensors, consisting of a large number of single sensor elements, each of which records a measured intensity level. In most digital cameras, the sensor array is covered with a patterned color filter mosaic having red, green, and blue regions in the [[Bayer filter]] arrangement, so that each sensor element can record the intensity of a single primary color of light. The camera interpolates the color information of neighboring sensor elements, through a process called [[demosaicing]], to create the final image. These sensor elements are often called \"pixels\", even though they only record 1 channel (only red, or green, or blue) of the final color image. Thus, two of the three color channels for each sensor must be interpolated and a so-called ''N-megapixel'' camera that produces an N-megapixel image provides only one-third of the information that an image of the same size could get from a scanner. Thus, certain color contrasts may look fuzzier than others, depending on the allocation of the primary colors (green has twice as many elements as red or blue in the Bayer arrangement).\n\n[[DxO Labs]] invented the [[Perceptual MegaPixel]] (P-MPix) to measure the sharpness that a camera produces when paired to a particular lens \u2013 as opposed to the MP a manufacturer states for a camera product which is based only on the camera's sensor. The new P-MPix claims to be a more accurate and relevant value for photographers to consider when weighing-up camera sharpness.<ref>http://www.dxomark.com/en/Reviews/Looking-for-new-photo-gear-DxOMark-s-Perceptual-Megapixel-can-help-you</ref> As of mid-2013, the [[Sigma 35mm F1.4 DG HSM]] mounted on a [[Nikon D800]] has the highest measured P-MPix. However, with a value of 23 MP, it still wipes-off more than one-third of the D800's 36.3 MP sensor.<ref>http://www.dxomark.com/index.php/Lenses/Camera-Lens-Ratings/Optical-Metric-Scores</ref>\n\nA camera with a full-frame image sensor, and a camera with an [[APS-C]] image sensor, may have the same pixel count (for example, 16&nbsp;MP), but the full-frame camera may allow better dynamic range, less noise, and improved low-light shooting performance than an APS-C camera. This is because the full-frame camera has a larger image sensor than the APS-C camera, therefore more information can be captured per pixel. A full-frame camera that shoots photographs at 36 megapixels has roughly the same pixel size as an APS-C camera that shoots at 16 megapixels.<ref>{{cite web |url=http://www.gizmag.com/camera-sensor-size-guide/26684/ |title=Camera sensor size: Why does it matter and exactly how big are they? |date=March 21, 2013}}</ref>\n\nOne new method to add Megapixels has been introduced in a [[Micro Four Thirds System]] camera which only uses 16MP sensor, but can produce 64MP RAW (40MP JPEG) by expose-shift-expose-shift the sensor a half pixel each time to both directions. Using a tripod to take level multi-shots within an instance, the multiple 16MP images are then generated into a unified 64MP image.<ref>{{cite web |url=http://www.dpreview.com/articles/5476551037/interview-with-setsuya-kataoka-from-olympus-om-d-high-resolution-mode |title=Soon, 40MP without the tripod: A conversation with Setsuya Kataoka from Olympus |author=Damien Demolder |accessdate=March 8, 2015 |date=February 14, 2015}}</ref>\n\n== See also ==\n{{Portal|Computer graphics}}\n{{div col||30em}}\n* [[Computer display standard]]\n* [[Dexel]]\n* [[Gigapixel image]]\n* [[Image resolution]]\n* [[Intrapixel and Interpixel processing]]\n* [[LCD crosstalk]]\n* [[PenTile matrix family]]\n* [[Pixel advertising]]\n* [[Pixel art]]\n* [[Pixel art scaling algorithms]]\n* [[Pixel aspect ratio]]\n* [[Point (typography)]]\n* [[Glossary of video terms]]\n* [[Voxel]]\n{{div col end}}\n\n==References==\n{{Reflist|2}}\n\n==External links==\n* [http://alvyray.com/Memos/CG/Microsoft/6_pixel.pdf A Pixel Is Not A Little Square]: Microsoft Memo by computer graphics pioneer Alvy Ray Smith.\n* [https://www.youtube.com/watch?v=D6n2Esh4jDY Video of talk on pixel history] at the [[Computer History Museum]]\n* [http://lurkertech.com/lg/pixelaspect/ Square and non-Square Pixels]: Technical info on pixel aspect ratios of modern video standards (480i, 576i, 1080i, 720p), plus software implications.\n* [http://www.gigapixel360.com 120 Megapixel is here now]: A lot of information about MegaPixel and Gigapixel.\n\n{{Photography}}\n\n[[Category:Computer graphics data structures]]\n[[Category:Digital geometry]]\n[[Category:Digital imaging]]\n[[Category:Digital photography]]\n[[Category:Display technology]]\n[[Category:Image processing]]\n[[Category:Television technology]]"}]}}}}