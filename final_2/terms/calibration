{"batchcomplete":"","query":{"normalized":[{"from":"calibration","to":"Calibration"}],"pages":{"47502":{"pageid":47502,"ns":0,"title":"Calibration","revisions":[{"contentformat":"text/x-wiki","contentmodel":"wikitext","*":"{{Redirect|Zeroing|method used by the U.S. government to calculate foreign antidumping duties|Zeroing (trade)}}\n\n'''Calibration''' in measurement technology and [[metrology]] is the comparison of [[measurement]] values delivered by a [[device under test]] with those of a [[Standard (metrology)|calibration standard]] of known accuracy. Such a standard could be another measurement device of known accuracy, a device generating the quantity to be measured such as a voltage, or a physical artefact, such as a metre ruler.\n\nThe outcome of the comparison can result in no significant error being noted on the device under test, a significant error being noted but no adjustment made, or an adjustment made to correct the error to an acceptable level. Strictly, the term calibration means just the act of comparison, and does not include any subsequent adjustment.\n\nThe calibration standard is normally traceable to a national standard held by a National Metrological Institute.\n\n==BIPM Definition==\nThe formal definition of calibration by the [[International Bureau of Weights and Measures]] (BIPM) is the following: \"Operation that, under specified conditions, in a first step, establishes a relation between the quantity values with measurement uncertainties provided by measurement standards and corresponding indications with associated measurement uncertainties (of the calibrated instrument or secondary standard) and, in a second step, uses this information to establish a relation for obtaining a measurement result from an indication.\"<ref name=metrology_terms>''[http://www.bipm.org/utils/common/documents/jcgm/JCGM_200_2008.pdf JCGM 200:2008 International vocabulary of metrology]'' \u2014 Basic and general concepts and associated terms (VIM)</ref>\n \nThis definition states that the calibration process is purely a comparison, but introduces the concept of [[Measurement uncertainty]] in relating the accuracies of the device under test and the standard.\n\n==Modern calibration processes==\nThe increasing need for known accuracy and uncertainty and the need to have consistent and comparable standards internationally has led to the establishment of National laboratories. In many countries a National Metrology Institute (NMI) will exist which will maintain primary standards of measurement (the main [[International System of Units|SI units]] plus a number of derived units) which will be used to provide [[traceability]] to customer's instruments by calibration.\n\nThe NMI supports the metrological infrastructure in that country (and often others) by establishing an unbroken chain, from the top level of standards to an instrument used for measurement.  Examples of National Metrology Institutes are [[National Physical Laboratory, UK|NPL]] in the [[UK]], [[NIST]] in the [[United States]], [[Physikalisch-Technische Bundesanstalt|PTB]] in [[Germany]] and many others.  Since the Mutual Recognition Agreement was signed it is now straightforward to take traceability from any participating NMI and it is no longer necessary for a company to obtain traceability for measurements from the NMI of the country in which it is situated., such as the [[National Physical Laboratory (United Kingdom)|National Physical Laboratory]] in the UK\n\n===Quality===\nTo improve the quality of the calibration and have the results accepted by outside organizations it is desirable for the calibration and subsequent measurements to be \"traceable\" to the internationally defined measurement units.  Establishing [[traceability]] is accomplished by a formal comparison to a [[Standard (metrology)|standard]] which is directly or indirectly related to national standards ( such as [[NIST]] in the USA), international standards, or [[certified reference materials]]. This may be done by national standards laboratories operated by the government or by private firms offering metrology services.\n\n[[Quality management system]]s call for an effective [[metrology]] system which includes formal, periodic, and documented calibration of all measuring instruments.  [[ISO 9000]]<ref name=\"iso9001\">ISO 9001: \"Quality manag systems \u2014 Requirements\" (2008), section 7.6.</ref> and [[ISO 17025]]<ref name=\"iso17025\">ISO 17025: \"General requirements for the competence of testing and calibration laboratories\" (2005), section 5.</ref> standards require that these traceable actions are to a high level and set out how they can be quantified.\n\nTo communicate the quality of a calibration the calibration value is often accompanied by a traceable uncertainty statement to a stated confidence level.  This is evaluated through careful uncertainty analysis.\nSome times a DFS (Departure From Spec) is required to operate machinery in a degraded state. Whenever this does happen, it must be in writing and authorized by a manager with the technical assistance of a calibration technician.\n\nMeasuring devices and instruments are categorized according to the physical quantities they are designed to measure. These vary internationally, e.g., [[National Institute of Standards and Technology|NIST]] 150-2G in the U.S.<ref>{{Cite journal|url = http://www.nist.gov/nvlap/upload/hb150-2g-1.pdf|title = Calibration Laboratories: Technical Guide for Mechanical Measurements|last = Faison|first = C. Douglas|date = March 2004|journal = NIST Handbook 150-2G|doi = |pmid = |access-date = 14 June 2015|last2 = Brickenkamp|first2 = Carroll S.|publisher = [[NIST]]}}</ref> and [[National Accreditation Board for Testing and Calibration Laboratories|NABL]]-141 in India.<ref>{{Cite web|url = http://www.fcriindia.com/national-training-2/metrology-pressure-thermal-electrotechnical-measurement-calibration/|title = Metrology, Pressure, Thermal & Eletrotechnical Measurement and Calibration|date = |accessdate = 14 June 2015|website = |publisher = Fluid Control Research Institute (FCRI), Ministry of Heavy Industries & Public Enterprises, Govt. of India|last = |first = |archiveurl = https://web.archive.org/web/20150614205726/http://www.fcriindia.com/national-training-2/metrology-pressure-thermal-electrotechnical-measurement-calibration/|archivedate = 14 June 2015}}</ref> Together, these standards cover instruments that measure various physical quantities such as [[electromagnetic radiation]] ([[RF probe]]s), time and frequency ([[intervalometer]]), [[ionizing radiation]] ([[Geiger counter]]), light ([[light meter]]), mechanical quantities ([[limit switch]], [[pressure gauge]], [[pressure switch]]), and, thermodynamic or thermal properties ([[thermometer]], [[temperature control]]ler). The standard instrument for each test device varies accordingly, e.g., a dead weight tester for pressure gauge calibration and a dry block temperature tester for temperature gauge calibration.\n\n==Instrument calibration prompts==\nCalibration may be required called for the following reasons:\n* a new instrument\n* after an instrument has been repaired or modified\n* when a specified time period has elapsed\n* when a specified usage (operating hours) has elapsed\n* before and/or after a critical measurement\n* after an event, for example\n** after an instrument has been exposed to a shock, [[vibration]], or physical damage, which might potentially have compromised the integrity of its calibration\n** sudden changes in weather\n* whenever observations appear questionable or instrument indications do not match the output of surrogate instruments\n* as specified by a requirement, e.g., customer specification, instrument manufacturer recommendation.\n\nIn general use, calibration is often regarded as including the process of '''adjusting''' the output or indication on a measurement instrument to agree with value of the applied standard, within a specified accuracy. For example, a [[thermometer]] could be calibrated so the error of indication or the correction is determined, and adjusted (e.g. via [[Operational definition#Temperature|calibration]] constants) so that it shows the true temperature in [[Celsius]] at specific points on the scale. This is the perception of the instrument's end-user. However, very few instruments can be adjusted to exactly match the standards they are compared to. For the vast majority of calibrations, the calibration process is actually the comparison of an unknown to a known and recording the results.\n\n==Basic calibration process==\n\n===Purpose and scope===\nThe calibration process begins with the design of the measuring instrument that needs to be calibrated. The design has to be able to \"hold a calibration\" through its calibration interval. In other words, the design has to be capable of measurements that are \"within [[engineering tolerance]]\" when used within the stated environmental conditions over some reasonable period of time.<ref name=\"HaiderAsif2011\">{{cite book|last1=Haider|first1=Syed Imtiaz|last2=Asif|first2=Syed Erfan|title=Quality Control Training Manual: Comprehensive Training Guide for API, Finished Pharmaceutical and Biotechnologies Laboratories|url=https://books.google.com/books?id=-djll_c9Z9MC&pg=PA49|date=16 February 2011|publisher=CRC Press|isbn=978-1-4398-4994-1|page=49}}</ref> Having a design with these characteristics increases the likelihood of the actual measuring instruments performing as expected.\nBasically,the purpose of calibration is for maintaining the quality of measurement as well as to ensure the proper working of particular instrument.\n\n===Frequency===\nThe exact mechanism for assigning tolerance values varies by country and industry type. The measuring equipment manufacturer generally assigns the measurement tolerance, suggests a calibration interval (CI) and specifies the environmental range of use and storage. The using organization generally assigns the actual calibration interval, which is dependent on this specific measuring equipment's likely usage level. The assignment of calibration intervals can be a formal process based on the results of previous calibrations. The standards themselves are not clear on recommended CI values:<ref>{{cite book|last1=Bare|first1=Allen|title=Simplified Calibration Interval Analysis|date=2006|publisher=NCSL International Workshop and Symposium, under contract with the Office of Scientific and Technical Information, U.S. Department of Energy|location=Aiken, SC|pages=1\u20132|url=http://sti.srs.gov/fulltext/2006/ms2006099.pdf|accessdate=28 November 2014}}</ref>\n:''[[ISO/IEC 17025|ISO 17025]]''<ref name=\"iso17025\" />\n::\"A calibration certificate (or calibration label) shall not contain any recommendation on the calibration interval except where this has been agreed with the customer. This requirement may be superseded by legal regulations.\u201d\n:''ANSI/NCSL Z540''<ref>{{cite web|title=ANSI/NCSL Z540.3-2006 (R2013)|url=http://www.ncsli.org/I/i/p/z3/c/a/p/NCSL_International_Z540.3_Standard.aspx?hkey=7de83171-16ff-416c-9182-94c8447fb300|publisher=The National Conference of Standards Laboratories (NCSL) International|accessdate=28 November 2014}}</ref>\n::\"...shall be calibrated or verified at periodic intervals established and maintained to assure acceptable reliability...\"\n:''[[ISO 9000#Contents of ISO 9001|ISO-9001]]''<ref name=\"iso9001\" />\n::\"Where necessary to ensure valid results, measuring equipment shall...be calibrated or verified at specified intervals, or prior to use...\u201d\n:''MIL-STD-45662A''<ref>{{cite web|title=Calibration Systems Requirements (Military Standard)|url=http://www.medivactech.com/revA.pdf|publisher=U.S. Department of Defense|accessdate=28 November 2014|location=Washington, DC|date=1 August 1998}}</ref>\n::\"... shall be calibrated at periodic intervals established and maintained to assure acceptable accuracy and reliability...Intervals shall be shortened or may be lengthened, by the contractor, when the results of previous calibrations indicate that such action is appropriate to maintain acceptable reliability.\"\n\n===Standards required and accuracy===\nThe next step is defining the calibration process. The selection of a standard or standards is the most visible part of the calibration process. Ideally, the standard has less than 1/4 of the measurement uncertainty of the device being calibrated. When this goal is met, the accumulated measurement uncertainty of all of the standards involved is considered to be insignificant when the final measurement is also made with the 4:1 ratio.<ref name=\"JablonskiBrezina2011\">{{Citation\n| editor1-last = Jab\u0142o\u0144ski | editor1-first = Ryszard\n| editor2-last = B\u0159ezina | editor2-first = Toma\u0161\n| series = Mechatronics: Recent Technological and Scientific Advances\n| last1 = Ligowski | first1 = M.\n| last2 = Jab\u0142o\u0144ski | first2 = Ryszard\n| last3 = Tabe | first3 = M.\n| title = Procedure for Calibrating Kelvin Probe Force Microscope\n| page = 227\n| year = 2011\n| isbn = 978-3-642-23244-2\n| doi = 10.1007/978-3-642-23244-2\n| id = {{LCCN|2011935381}}\n}}</ref> This ratio was probably first formalized in Handbook 52 that accompanied MIL-STD-45662A, an early US Department of Defense metrology program specification. It was 10:1 from its inception in the 1950s until the 1970s, when advancing technology made 10:1 impossible for most electronic measurements.<ref>{{cite book|title=Military Handbook: Evaluation of Contractor's Calibration System|url=http://www.barringer1.com/mil_files/MIL-HDBK-52.pdf|publisher=U.S. Department of Defense|page=7|accessdate=28 November 2014|date=17 August 1984}}</ref>\n\nMaintaining a 4:1 accuracy ratio with modern equipment is difficult. The test equipment being calibrated can be just as accurate as the working standard.<ref name=\"JablonskiBrezina2011\" /> If the accuracy ratio is less than 4:1, then the calibration tolerance can be reduced to compensate. When 1:1 is reached, only an exact match between the standard and the device being calibrated is a completely correct calibration. Another common method for dealing with this capability mismatch is to reduce the accuracy of the device being calibrated.\n\nFor example, a gage with 3% manufacturer-stated accuracy can be changed to 4% so that a 1% accuracy standard can be used at 4:1.  If the gage is used in an application requiring 16% accuracy, having the gage accuracy reduced to 4% will not affect the accuracy of the final measurements. This is called a limited calibration. But if the final measurement requires 10% accuracy, then the 3% gage never can be better than 3.3:1. Then perhaps adjusting the calibration tolerance for the gage would be a better solution. If the calibration is performed at 100 units, the 1% standard would actually be anywhere between 99 and 101 units. The acceptable values of calibrations where the test equipment is at the 4:1 ratio would be 96 to 104 units, inclusive. Changing the acceptable range to 97 to 103 units would remove the potential contribution of all of the standards and preserve a 3.3:1 ratio. Continuing, a further change to the acceptable range to 98 to 102 restores more than a 4:1 final ratio.\n\nThis is a simplified example. The mathematics of the example can be challenged. It is important that whatever thinking guided this process in an actual calibration be recorded and accessible. Informality contributes to [[tolerance stacks]] and other difficult to diagnose post calibration problems.\n\nAlso in the example above, ideally the calibration value of 100 units would be the best point in the gage's range to perform a single-point calibration. It may be the manufacturer's recommendation or it may be the way similar devices are already being calibrated. Multiple point calibrations are also used. Depending on the device, a zero unit state, the absence of the phenomenon being measured, may also be a calibration point.  Or zero may be resettable by the user-there are several variations possible. Again, the points to use during calibration should be recorded.\n\nThere may be specific connection techniques between the standard and the device being calibrated that may influence the calibration. For example, in electronic calibrations involving analog phenomena, the impedance of the cable connections can directly influence the result.\n\n===Manual and automatic calibrations===\nCalibration methods for modern devices can be both manual and automatic, depending on what kind of device is being calibrated.\n[[Image:US Navy 040830-N-4565G-002 Fireman Joshua Morgan, of Waco, Texas, calibrates a Engineering pressure gage.jpg|thumb|Manual calibration - US serviceman calibrating a temperature gauge. The device under test is on his left and the test standard on his right.]]\n\n====Manual====\nThe first picture  shows a U.S. Navy Airman performing a manual calibration procedure on a pressure test gauge. The procedure is complex,<ref>{{cite book|title=Procedure for calibrating pressure gauges (USBR 1040)|publisher=U.S. Department of the Interior, Bureau of Reclamation|pages=70\u201373|url=http://www.usbr.gov/pmts/geotech/rock/EMpart_2/USBR1040.pdf|accessdate=28 November 2014}}</ref> but overall it involves the following: (i) [[Depressurization|depressurizing]] the system, and turning the screw, if necessary, to ensure that the needle reads zero, (ii) fully pressurizing the system and ensuring that the needle reads maximum, within acceptable tolerances, (iii) replacing the gauge if the error in the calibration process is beyond tolerance, as this may indicate signs of failure such as [[corrosion]] or [[Fatigue (material)|material fatigue]].\n\n====Automatic calibration====\n[[Image:US Navy 040829-N-7884F-006 Machinist Mate 2nd Class Frank Cundiff completes calibration testing on pressure gauges using the 3666C auto pressure calibrator.jpg|thumb|Automatic calibration - A U.S. serviceman using a 3666C auto pressure calibrator]]\nThe second picture shows the use of a 3666C automatic pressure calibrator,<ref>{{cite web|title=KNC Model 3666 Automatic Pressure Calibration System|url=http://www.kingnutronics.com/Model%203666%20Automatic%20Pressure%20Calibration%20System.pdf|publisher=King Nutronics Corporation|accessdate=28 November 2014}}</ref> which is a device that consists of a control unit housing the electronics that drive the system, a pressure intensifier used to compress a gas such as [[Nitrogen]], a [[pressure transducer]] used to detect desired levels in a [[hydraulic accumulator]], and accessories such as [[Trap (plumbing)|liquid traps]] and gauge [[Piping and plumbing fittings|fittings]].\n\n===Process description and documentation===\nAll of the information above is collected in a calibration procedure, which is a specific [[test method]]. These procedures capture all of the steps needed to perform a successful calibration. The manufacturer may provide one or the organization may prepare one that also captures all of the organization's other requirements. There are clearinghouses for calibration procedures such as the Government-Industry Data Exchange Program (GIDEP) in the United States.\n\nThis exact process is repeated for each of the standards used until transfer standards, [[certified reference materials]] and/or natural physical constants, the measurement standards with the least uncertainty in the laboratory, are reached. This establishes the [[traceability]] of the calibration.\n\nSee [[Metrology]] for other factors that are considered during calibration process development.\n\nAfter all of this, individual instruments of the specific type discussed above can finally be calibrated. The process generally begins with a basic damage check. Some organizations such as nuclear power plants collect \"as-found\" calibration data before any [[Planned maintenance|routine maintenance]] is performed. After routine maintenance and deficiencies detected during calibration are addressed, an \"as-left\" calibration is performed.\n\nMore commonly, a calibration technician is entrusted with the entire process and signs the calibration certificate, which documents the completion of a successful calibration.\nThe basic process outlined above is a difficult and expensive challenge. The cost for ordinary equipment support is generally about 10% of the original purchase price on a yearly basis, as a commonly accepted [[rule-of-thumb]]. Exotic devices such as [[scanning electron microscope]]s, [[gas chromatograph]] systems and [[laser]] [[interferometer]] devices can be even more costly to maintain.\n\nThe 'single measurement' device used in the basic calibration process description above does exist. But, depending on the organization, the majority of the devices that need calibration can have several ranges and many functionalities in a single instrument. A good example is a common modern [[oscilloscope]]. There easily could be 200,000 combinations of settings to completely calibrate and limitations on how much of an all inclusive calibration can be automated.\n\n[[Image:F18NARack.JPG|thumb|An instrument rack with tamper-indicating seals]]\n\nTo prevent unauthorised access to an instrument tamper-proof seals are usually applied after calibration. The picture of the oscilloscope rack shows these, and prove that the instrument has not been removed since it was last calibrated as they will possible unauthorised to the adjusting elements of the instrument. There also are labels showing the date of the last calibration and when the calibration interval dictates when the next one is needed. Some organizations also assign unique identification to each instrument to standardize the record keeping and keep track of accessories that are integral to a specific calibration condition.\n\nWhen the instruments being calibrated are integrated with computers, the integrated computer programs and any calibration corrections are also under control.\n\n==Historical development==\n{{Main article|History of measurement}}\n\n===Origins===\nThe words \"calibrate\" and \"calibration\" entered the [[English language]] as recently as the [[American Civil War]],<ref>http://dictionary.reference.com/browse/calibrate</ref> in descriptions of [[artillery]], thought to be derived from a measurement of the calibre of a gun.\n\nSome of the [[History of measurement#Earliest known systems|earliest known systems of measurement]] and calibration seem to have been created between the ancient civilizations of [[Egyptian Civilization|Egypt]], [[Mesopotamia]] and the [[Indus Valley Civilization|Indus Valley]], with excavations revealing the use of angular gradations for construction.<ref name=\"Baber1996\">{{cite book |last=Baber |first=Zaheer |date=1996 |title=The Science of Empire: Scientific Knowledge, Civilization, and Colonial Rule in India |publisher=SUNY Press |isbn=978-0-7914-2919-8 |pages=23\u201324 |url=https://books.google.com/books?id=ucDBJSxaCPYC&pg=PA14}}</ref> The term \"calibration\" was likely first associated with the precise division of linear distance and angles using a [[dividing engine]] and the measurement of gravitational [[mass]] using a [[weighing scale]]. These two forms of measurement alone and their direct derivatives supported nearly all commerce and technology development from the earliest civilizations until about AD 1800.<ref name=\"FranceschiniGaletto2011\">{{cite book |last1=Franceschini |first1=Fiorenzo |last2=Galetto |first2=Maurizio |last3=Maisano |first3=Domenico |last4=Mastrogiacomo |first4=Luca |last5=Pralio |first5=Barbara |date=6 June 2011 |title=Distributed Large-Scale Dimensional Metrology: New Insights |isbn=978-0-85729-543-9 |publisher=Springer Science & Business Media |pages=117\u2013118 |url=https://books.google.com/books?id=bIwbFtXyMcMC&pg=PA117}}</ref>\n\n===Calibration of weights and distances ({{c.|1100 CE}})===\n{{see also|Weights and Measures Act}}\n\n[[Image:Avery postal scale.JPG|thumb|upright|right|An example of a [[weighing scale]] with a \u00bd ounce calibration error at zero. This is a \"zeroing error\" which is inherently indicated, and can normally be adjusted by the user, but may be due to the string and rubber band in this case]]\nEarly measurement devices were ''direct'', i.e. they had the same units as the quantity being measured. Examples include length using a yardstick and mass using a weighing scale. At the beginning of the twelfth century, during the reign of Henry I (1100-1135), it was decreed that a yard be \"the distance from the tip of the King's nose to the end of his outstretched thumb.\"<ref name=\"Ackroyd2012\">{{cite book|last=Ackroyd|first=Peter|title=Foundation: The History of England from Its Earliest Beginnings to the Tudors|url=https://books.google.com/books?id=Z2XHs80O0OEC&pg=PT133|date=16 October 2012|publisher=St. Martin's Press|isbn=978-1-250-01367-5|pages=133\u2013134}}</ref> However, it wasn't until the reign of Richard I (1197) that we find documented evidence.<ref name=\"BlandTawney1919\">{{cite book|last1=Bland|first1=Alfred Edward|last2=Tawney|first2=Richard Henry|title=English Economic History: Select Documents|url=https://books.google.com/books?id=bSO6emWmBuAC&pg=PA154|year=1919|publisher=Macmillan Company|pages=154\u2013155}}</ref>\n:''Assize of Measures''\n:\"Throughout the realm there shall be the same yard of the same size and it should be of iron.\"\n\nOther standardization attempts followed, such as the [[Magna Carta]] (1225) for liquid measures, until the [[M\u00e8tre des Archives]] from France and the establishment of the [[Metric system]].\n\n===The early calibration of pressure instruments===\n[[Image:Utube.PNG|thumb|Left|upright|100px|Direct reading design of a U-tube manometer]]One of the earliest pressure measurement devices was the [[Evangelista Torricelli#Barometer|Mercury barometer, credited to Torricelli]] (1643),<ref name=tilford1992pressure>{{cite journal|last1=Tilford|first1=Charles R|title=Pressure and vacuum measurements|journal=Physical methods of chemistry|date=1992|pages=106\u2013173|url=http://www.glb.nist.gov/calibrations/upload/pmc-2.pdf|accessdate=28 November 2014}}</ref> which read atmospheric pressure using [[Mercury (element)|Mercury]]. Soon after, water-filled [[manometer]]s were designed. All these would have linear calibrations using gravimetric principles, where the difference in levels was proportional to pressure. The normal units of measure would be the convenient inches of mercury or water.\n\nIn the direct reading hydrostatic manometer design on the right,  applied pressure P<sub>a</sub> pushes the liquid down the right side of the manometer U-tube, while a length scale next to the tube measures the difference of levels. The resulting height difference \"H\" is a direct measurement of the pressure or vacuum with respect to [[atmospheric pressure]]. In the absence of differential pressure both levels would be equal, and this would be used as the zero point. \n  \nThe [[Industrial Revolution]] saw the adoption of \"indirect\" pressure measuring devices, which were more practical than the manometer. <ref name=\"FridmanSabak2011\">{{cite book |last1=Fridman |first1=A. E. |last2=Sabak |first2=Andrew |last3=Makinen |first3=Paul |date=23 November 2011 |title=The Quality of Measurements: A Metrological Reference |publisher=Springer Science & Business Media |isbn=978-1-4614-1478-0 |pages=10\u201311 |url=https://books.google.com/books?id=kyX-1VzPokQC&pg=PA111}}</ref>\nAn example is in high pressure (up to 50 psi) steam engines, where mercury was used to reduce the scale length to about 60 inches, but such a manometer was expensive and prone to damage. <ref name=cusco1998guide>{{cite book|last1=Cusc\u00f3|first1=Laurence|title=Guide to the Measurement of Pressure and Vacuum|date=1998|publisher=The Institute of Measurement and Control|location=London|isbn=0 904457 29 X|page=5}}</ref> This stimulated the development of indirect reading instruments, of which the [[Bourdon tube]] invented by [[Eugene Bourdon]] is a notable example. \n{{multiple image\n| footer    = Indirect reading design showing a Bourdon tube from the front (left) and the rear (right).\n| width     = 100\n| image1    = WPGaugeFace.jpg\n| alt1      = Indirect reading design showing a Bourdon tube from the front\n| image2    = WPPressGaugeMech.jpg\n| alt2      = Indirect reading design showing a Bourdon tube from the rear\n}}\n\nIn the front and back views of a Bourdon gauge on the right, applied pressure at the bottom fitting reduces the curl on the flattened pipe proportionally to pressure. This moves the free end of the tube which is linked to the pointer. The instrument would be calibrated against a manometer, which would be the calibration standard. For measurement of indirect quantities of pressure per unit area, the calibration uncertainty would be dependent on the density of the manometer fluid, and the means of measuring the height difference. From this other units such as pounds per square inch could be inferred and marked on the scale.\n\n==See also==\n{{div col|3}}\n*[[Calibration curve]]\n*[[Calibrated geometry]]\n*[[Calibration (statistics)]]\n*[[Color calibration]] \u2013 used to calibrate a [[computer monitor]] or display.\n*[[Deadweight tester]]\n*[[EURAMET]] Association of European NMIs\n*[[Measurement Microphone Calibration]]\n*[[Measurement uncertainty]]\n*[[Musical tuning]] \u2013 tuning, in music, means calibrating musical instruments into playing the right pitch.\n*[[Precision measurement equipment laboratory]]\n*[[Scale test car]] \u2013 a device used to calibrate [[Truck scale|weighing scales]] that weigh [[railroad car]]s.\n*[[Systems of measurement]]\n{{div col end}}\n\n==References==\nCrouch, Stanley & Skoog, Douglas A. (2007). Principles of Instrumental Analysis. Pacific Grove: Brooks Cole. ISBN 0-495-01201-7.\n{{reflist}}\nIS:ISO:ISI:17025:2005\n\n==External links==\n* [http://www.mastercalibrators.asn.au Master Calibrators Association - Industry Experts on Electrical Calibration, Adjustment and Metrology]\n\n[[Category:Accuracy and precision]]\n[[Category:Standards]]\n[[Category:Measurement]]\n[[Category:Metrology]]"}]}}}}